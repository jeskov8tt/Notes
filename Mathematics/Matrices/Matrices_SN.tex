\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[italicdiff]{physics}
\usepackage{enumerate}
\usepackage{microtype}
\DisableLigatures{encoding= *, family=*}
\usepackage{titlesec}
\usepackage{xfrac}
\setcounter{secnumdepth}{4}
\usepackage{xcolor}
\usepackage[bookmarks=false]{hyperref}
\usepackage{mathtools}
\usepackage{bigints}
\hypersetup{
    colorlinks=true,
    linkcolor=[RGB]{59 108 209},
    urlcolor=[RGB]{59 108 209}
}
\urlstyle{same}

\newcommand{\notimplies}{\;\not\!\!\!\implies}
\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\title{Matrices}
\author{}
\date{}

\begin{document}
\maketitle

\section{Types of Matrices}

\subsection{Row Matrix or Row Vector}

A matrix which contains only \textbf{One Row}.

$$A= {\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{1 \cross n}$$

\subsection{Column Matrix or Column Vector}

A matrix which contains only \textbf{One Column}.

$$A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{n \cross 1} $$

\subsection{Rectangular Matrix}

A matrix in which \textbf{no. of rows is not equal to no. of columns}.

$$A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix} }_{m \cross n}, m \not= n $$

\subsection{Square Matrix}

A matrix in which \textbf{no. of rows is equal to no. of columns}.

$$A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{n \cross n} $$

\subsection{Diagonal Matrix}

A \textbf{square matrix} in which \textbf{all non-diagonal entries are zero}.

$$A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{n \cross n}, a_{ij}=0 \hspace{2mm} \forall \hspace{2mm} i \not= j$$

\subsection{Scalar Matrix}

A \textbf{Diagonal Matrix} in which \textbf{all diagonal entries are equal}.

$$A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{n \cross n}, a_{ij} = \begin{cases}
        a & i=j, a \in \mathbb{C} \\
        0 & i \not= j
    \end{cases} $$

\subsection{Identity or Unit Matrix}

A \textbf{Diagonal Matrix} in which \textbf{all diagonal entries equal 1}.

$$A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{n \cross n}, a_{ij}=\begin{cases}
        1 & i=j       \\
        0 & i \not= j
    \end{cases} $$

\subsection{Singleton Matrix}

A matrix which consists of \textbf{only one element}.

$$A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{1 \cross 1} $$

\subsection{Triangular Matrix}

\begin{enumerate}
    \item \textbf{Upper Triangular Matrix}

          A \textbf{Square Matrix} in which \textbf{all elements below principal diagonal are zero}.

          $$A={\begin{bmatrix}
                  a_{ij}
              \end{bmatrix}}_{n \cross n}, a_{ij}=0 \hspace{2mm} \forall \hspace{2mm} i >j $$

    \item \textbf{Lower Triangular Matrix}

          A \textbf{Square Matrix} in which \textbf{all elements above principal diagonal are zero}.
          $$A={\begin{bmatrix}
                  a_{ij}
              \end{bmatrix}}_{n \cross n}, a_{ij}=0 \hspace{2mm} \forall \hspace{2mm} i<j $$
\end{enumerate}

\subsection{Horizontal Matrix}

A matrix in which \textbf{no. of rows is less than no. of columns}

$$A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{m \cross n}, m <n  $$

\subsection{Vertical Matrix}

A matrix in which \textbf{no. of rows is greater than no. of columns}.

$$A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{m \cross n}, m>n $$

\subsection{Null or Zero Matrix}

A matrix in which \textbf{all elements equal zero.}

$$O={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{m \cross n}, a_{ij}=0 \hspace{2mm} \forall \hspace{2mm} i,j $$

\subsection{Sub-Matrix}

A matrix which is obtained from a given matrix \textbf{by deleting any no. of rows and no. of columns}.

e.g. ${\begin{bmatrix}
                3  & 4 \\
                -2 & 5
            \end{bmatrix}}_{2 \cross 2}$ is a sub-matrix of ${\begin{bmatrix}
                8 & 9  & 5 \\
                2 & 3  & 4 \\
                3 & -2 & 5
            \end{bmatrix}}_{3 \cross 3} $

\subsection{Comparable Matrices}

Two Matrices $A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{m \cross n} $ and $B= {\begin{bmatrix}
        b_{ij}
    \end{bmatrix}}_{p \cross q} $ are \textbf{comparable}, \\if \textbf{$m=p \wedge n = q$}


\subsection{Idempotent Matrix}

A square matrix whose square equals itself.

$$A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{n \cross n}, A^2=A $$

\begin{itemize}

    \item   For a Idempotent Matrix, $A^n=A \hfill \forall \hspace{2mm} n \in \mathbb{Z^+}$
\end{itemize}
\subsection{Periodic Matrix}

A square matrix $A$ is called periodic,

If $$A^{k+1}=A, k \in \mathbb{Z^+} $$
\begin{itemize}

    \item  The least value of all such $k$ is called the period of matrix $A$.

    \item   Period of an Idempotent Matrix is $1$.
\end{itemize}
\subsection{Nilpotent Matrix}

A square matrix $A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{n \cross n} $ which

satisfies $$A^k=O, A^{k-1} \not=O $$

\subsection{Involutory Matrix}

A square matrix $A$ which

satisfies, $$A^2=I$$
\begin{itemize}

    \item   For Involutory Matrix, $A=A^{-1}$
\end{itemize}
\subsection{Symmetric Matrix}

A square matrix $A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{n \cross n} $ in which $A=A^T$.
\begin{itemize}


    \item In a Symmetric Matrix, elements situated at equal distance from the diagonal are equal.
    \item For any Square Matrix $A$ with $a_{ij} \in \mathbb{R}$, then $A+A^T$ is symmetric.
    \item If $A$ is a Square Matrix, then $AA^T$ and $A^TA$ are symmetric matrices.
    \item All positive integral powers of a symmetric matrix are symmetric.

          $$\textit{If } A={\begin{bmatrix}
                  a_{ij}
              \end{bmatrix}}_{n \cross n}, a_{ij}=a_{ji} \textit{ Then } A^n={\begin{bmatrix}
                  \alpha_{ij}
              \end{bmatrix}}_{n \cross n}, \alpha_{ij}=\alpha_{ji} $$
    \item If $A$ be a symmetric matrix and $B$ be a square matrix of order that of $A$, then $-A, kA, A^T, A^{-1},A^n, B^TAB $ are also symmetric. $n \in \mathbb{N}, k \in \mathbb{C} $
    \item Let $A$ and $B$ be two symmetric matrices of same order, then

          \begin{enumerate}[a.]
              \item $A \pm B, AB+BA $ are symmetric.
              \item $AB-BA$ is skew-symmetric.
              \item $AB$ is symmetric iff, $AB=BA$
          \end{enumerate}
\end{itemize}

\subsection{Skew-Symmetric Matrix}

A Square Matrix $A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{n \cross n} $ in which $a_{ij}=-a_{ji} \hspace{2mm} \forall \hspace{2mm} i,j$
\begin{itemize}
    \item In a Skew-Symmetric Matrix all diagonal entries are \textbf{zero.}
    \item In a Skew-Symmetric Matrix, the elements situated at equal distance from the diagonal are equal in magnitude but opposite in sign.
    \item If $A$ is a Skew-Symmetric Matrix, then $Tr(A)=0$
    \item For any Square Matrix $A$ with $a_{ij}\in \mathbb{R} $, then $A-A^T$ is a Skew-Symmetric Matrix.
    \item Every Square Matrix can be uniquely expressed as the sum of a symmetric and skew-symmetric matrix.

          $$A=\dfrac{1}{2} \left(A+A^T\right)+\dfrac{1}{2} \left(A-A^T\right)$$
    \item All positive odd integral powers of a skew-symmetric matrix are skew-symmetric.
          $$\textit{ If } A={\begin{bmatrix}
                  a_{ij}
              \end{bmatrix}}_{m \cross m}, a_{ij}=-a_{ji} \textit{ Then } A^{2n+1}={\begin{bmatrix}
                          \alpha_{ij}
                      \end{bmatrix}}_{m \cross m}, \alpha_{ij}=-\alpha_{ji}, n\in \mathbb{N} $$
    \item All positive even integral powers of a skew-symmetric matrix are symmetric.
          $$\textit{ If } A={\begin{bmatrix}
                  a_{ij}
              \end{bmatrix}}_{m \cross m}, a_{ij}=-a_{ji} \textit{ Then } A^{2n}={\begin{bmatrix}
                          \alpha_{ij}
                      \end{bmatrix}}_{m \cross m}, \alpha_{ij}=\alpha_{ji}, n \in \mathbb{N}  $$
    \item If $A$ be a skew-symmetric matrix and $B$ be a square matrix of order that of $A$ then, $kA, B^TAB$ are also skew-symmetric.
    \item Let $A$ and $B$ be two skew-symmetric matrices of same order, then
          \begin{enumerate}[a.]
              \item $A \pm B, AB-BA $ are skew-symmetric.
              \item $AB+BA$ is symmetric
          \end{enumerate}
    \item Let $A$ be skew-symmetric matrix and $C$ be column vector, \\then $C^TAC=O$
\end{itemize}

\subsection{Orthogonal Matrix}

A square matrix whose transpose is its inverse.

$$AA^{T}=I$$
\begin{itemize}
    \item $A^{-1}=A^T $
    \item Let $A$ and $B$ be Orthogonal, then $AB$ is also Orthogonal.
    \item If $A$ is orthogonal, then $A^{-1}, A^T $ are also Orthogonal.
\end{itemize}

\subsection{Hermitian Matrix}
A square matrix which is equal to its conjugate transpose matrix.

$$\textit{ If } A_{n \cross n}=A_{n \cross n}^\theta \hspace{2mm} \vee A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{n \cross n}, a_{ij}=\overline{a_{ji}}, \textit{ Then A is Hermitian } $$

\begin{itemize}
    \item The diagonal entries of a Hermitian Matrix are $\mathbb{R}$
    \item Elements situated at equal distance from diagonal are complex conjugate of each other.
    \item Let $A$ be any square matrix, then $A+A^\theta$ is a Hermitian Matrix.
    \item If $A$ be a square matrix, then $AA^\theta, A^\theta A$ are Hermitian.
    \item If $A$ is Hermitian Matrix, then
          \begin{enumerate}[a.]
              \item $iA$ is Skew-Hermitian Matrix. $\hfill i^2=-1$
              \item $\overline{A}$ is also a Hermitian Matrix.
              \item $kA$ is Hermitian Matrix. $\hfill k \in \mathbb{R} $
              \item If $A, B$ are Hermitian Matrices of same order, then
                    \begin{enumerate}[a.]
                        \item $k_{1}A+k_{2}B $ is also Hermitian. $\hfill k_{1}, k_{2} \in \mathbb{R} $
                        \item $AB$ is also Hermitian if $AB=BA$
                        \item $AB+BA$ is Hermitian.
                        \item $AB-BA$ is Skew-Hermitian.
                    \end{enumerate}
          \end{enumerate}
\end{itemize}

\subsection{Skew-Hermitian Matrix}
A square matrix which is equal to negative of its conjugate transpose matrix.

$$\textit{ If } A_{n \cross n}=-A_{n \cross n}^\theta \hspace{2mm} \vee \hspace{2mm} A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{n \cross n}, a_{ij}=-\overline{a_{ji}}, \textit{ Then A is Skew-Hermitian } $$
\begin{itemize}
    \item All diagonal entries of a Skew-Hermitian Matrix are purely imaginary or zero.
    \item In a Skew-Hermitian Matrix elements situated at equal distance from the diagonal only differ in the sign of real part.
    \item Let $A$ be any square matrix, then $A-A^\theta$ is a Skew-Hermitian Matrix.
    \item Every square matrix can be uniquely expressed as the sum of a Hermitian and a Skew-Hermitian Matrix.
          $$A_{n \cross n}=\dfrac{1}{2}\left(A+A^\theta\right)+\dfrac{1}{2}\left(A-A^\theta\right) $$
    \item If $A$ is a Skew-Hermitian Matrix, then
          \begin{enumerate}[a.]
              \item $iA$ is a Hermitian Matrix. $\hfill i^2=-1$
              \item $\overline{A}$ is also Skew-Hermitian.
              \item $kA$ is Skew-Hermitian. $\hfill k \in \mathbb{R}$
          \end{enumerate}
    \item If $A, B$ are two Skew-Hermitian Matrices of same order, then $k_{1}A +k_{2}B $ is also Skew-Hermitian.
\end{itemize}

\subsection{Unitary Matrix}
A Square Matrix whose inverse is its conjugate transpose matrix.
$$AA^\theta=I \implies A^{-1}=A^\theta $$
\begin{itemize}
    \item If $A $ and $B $ are unitary, then $AB$ is also unitary.
    \item If $A$ is unitary, then $A^{-1}, A^T $ are also unitary.
\end{itemize}

\subsection{Equivalent Matrices}
Two matrices are said to be equivalent if one is obtained from the other by elementary operations.
$$A \sim B, \textit{A is equivalent to B} $$

\begin{itemize}
    \item If $A \sim B \hspace{2mm} \exists \hspace{2mm}$ invertible matrices $P$ and $Q$ such that $B=PAQ$
    \item Every invertible square matrix can be expressed as the product of elementary matrices.
\end{itemize}

\section{Trace of a Matrix}
The \textbf{sum of all diagonal entries} of a \textbf{square matrix} is called its \textbf{Trace}.

$$A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{n \cross n}, Tr(A)=\displaystyle \sum _{i=1}^{n} a_{ii} $$

\subsection{Properties of Trace of a Matrix}

Let $A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{n \cross n}, B={\begin{bmatrix}
                b_{ij}
            \end{bmatrix}}_{n \cross n}, k \in \mathbb{C} $
\begin{enumerate}[i.]
    \item $Tr(kA)=k \cdot Tl(A) $
    \item $Tr(A \pm B)= Tr(A) \pm Tr(B)$
    \item $Tr(AB)=Tr(BA)$
    \item $Tr(A)=Tr(A')$
    \item $Tr(I_{n})=n$
    \item $Tr(AB) \not= Tr(A) \cdot Tr(B)$
    \item $Tr(A)=Tr(CAC^{-1}) C$ is a non-singular square matrix of order $n$.
\end{enumerate}

\section{Determinant of a Square Matrix}
Let $A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{n \cross n} $. The determinant formed by the elements of $A$ is said to be the determinant of matrix $A= det(A)= |A|$

\subsection*{Important Points}
\begin{enumerate}[a.]
    \item $det\left(\displaystyle\prod_{r=1}^{n} A_{r}\right)=\displaystyle\prod_{r=1}^{n} det(A_{r}) $
    \item $det(kA)=k^n det(A) \hfill k \in \mathbb{C}, n = \text{order of $A$} $
    \item $det(A) \text{ exists} \iff A$ is a square matrix.
    \item $det(A)=det(A^T) $
    \item If $A$ is Orthogonal Matrix, then $det(A)=\pm 1 $
    \item If $A$ is Skew-Symmetric Matrix of odd order, then $det(A)=0$
    \item If $A$ is Skew-Symmetric Matrix of even order, then $det(A)$ is a perfect square.
    \item If $A=diag(a_{1}, a_{2}, \ldots , a_{n})$, then $det(A)=\displaystyle\prod_{r=1}^{n}a_{r}$
    \item $det(A^{-1})=\dfrac{1}{det(A)}$
\end{enumerate}

\subsection*{Singular (Invertible) and\\ non-singular (Non-Invertible) Matrix}

If for $A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{n \cross n}, det(A)=0 $ then $A$ is called a \textbf{}{singular} matrix. \\If $det(A) \not= 0$ then it is called \textbf{non-singular} matrix.

\section{Equal Matrices}
Two matrices are equal, iff
\begin{enumerate}[i.]
    \item They are of same order.
    \item The elements in the corresponding positions of the two matrices are equal.

          Let $A={\begin{bmatrix}
                  a_{ij}
              \end{bmatrix}}_{m \cross n} $ and $B={\begin{bmatrix}
                  b_{ij}
              \end{bmatrix}}_{p \cross q} $

          Now, $A=B$ iff, $m=p, n=q, a_{ij}=b_{ij} \hspace{2mm} \forall \hspace{2mm} i,j$
\end{enumerate}

\section{Algebra of Matrices}

\subsection{Addition}
If $A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{m \cross n} $ and $B={\begin{bmatrix}
        b_{ij}
    \end{bmatrix}}_{m \cross n} $
\newline \newline
Then, $$A+B= {\begin{bmatrix}
        a_{ij}+b_{ij}
    \end{bmatrix}}_{m \cross n} $$

\textbf{Properties of Matrix Addition}

\begin{enumerate}[1.]
    \item Addition of Matrices is \textbf{commutative}.
          $$A+B= B+A$$

    \item Addition of Matrices is \textbf{associative}.
          $$(A+B)+C=A+(B+C)$$

    \item Additive Identity is $O$ (Null Matrix)

          $$A+O=A$$

    \item Additive Inverse is $-A$

          $$A+ (-A)= O$$

    \item Cancellation law

          $A+B=A+C \implies B=C$
\end{enumerate}
\subsection{Multiplication}

\subsubsection{Scalar Multiplication}
The  Matrix obtained by \textbf{multiplying every element of a matrix with a scalar} is called the scalar multiple of that matrix.

Let $A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{m \cross n}, k \in \mathbb{C}$

Now, $$kA={\begin{bmatrix}
        k \cdot a_{ij}
    \end{bmatrix}}_{m \cross n} $$

\subsubsection{Matrix Multiplication}

\textbf{Conformable for Multiplication}

Two matrices $A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{m \cross n}, B={\begin{bmatrix}
                b_{ij}
            \end{bmatrix}}_{p \cross q} $ are conformable for the product $AB$, if the no. of columns in the pre-factor ($A$) is equal to the no. of rows in the post-factor ($B$)

$$AB \textit{ exists iff } n=p $$

\textbf{Multiplication of Matrices}


Let $A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{m \cross n}, B={\begin{bmatrix}
                b_{ij}
            \end{bmatrix}}_{n \cross q}, C= {\begin{bmatrix}
                c_{ij}
            \end{bmatrix}}_{m \cross q} \textit{ s.t. } AB=C $

$\therefore c_{ij}=\displaystyle\sum_{r=1}^{n} a_{ir} \cdot b_{rj} \hfill \textit{Row by Column Multiplication}$

\subsubsection{Properties of Matrix Multiplication}
\begin{enumerate}[i.]
    \item Matrix Multiplication is not commutative.
          $$AB \not= BA$$
          \begin{itemize}
              \item   If $AB=BA$ then matrices $A, B$ are said to commute.

              \item    If $AB=-BA$ then matrices $A, B$ are said to anti-commute.

              \item  Diagonal Matrices of same order always commute.
          \end{itemize}
    \item Matrix Multiplication is associative.
          $$A(BC)=(AB)C$$

    \item If $A={\begin{bmatrix}
                  a_{ij}
              \end{bmatrix}}_{m \cross n} $, then $I_{m} \, A=A=AI_{n} $

    \item Matrix Multiplication is distributive.

          $$A(B+C)=AB+AC$$
          Note: $A(B+C) \not= AB+CA$

    \item If product of two matrices is a null matrix then it is not necessary that one of the matrices is also a null matrix.

          $$AB=O \notimplies A=O \vee B=O$$

          If $AB=O$ then $A, B$ are divisors of $O$

          $AB=O \implies det(AB)=0 \implies det(A) \cdot det(B)=0$

    \item $AO=O$

    \item $I^m=I \hfill \forall \hspace{2mm} m \in \mathbb{Z^+}$

    \item if $A={\begin{bmatrix}
                  a_{ij}
              \end{bmatrix}}_{n \cross n} $ \\

          then $\underbrace{A \cdot A \cdot A \ldots A }_{m \text{ times}}=A^m \hfill \forall \hspace{2mm} m \in \mathbb{Z^+}$
\end{enumerate}

\section{Pre-Multiplication and Post-Multiplication of Matrices}
The Matrix $AB$ is the matrix $B$ \textbf{pre-multiplied} by $A$ and the matrix $BA$ is the matrix B \textbf{post-multiplied} by $A$.

\section{Transpose of a Matrix}

The matrix obtained by \textbf{interchanging rows and columns}.

$$A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{m \cross n}, A^T={\begin{bmatrix}
        a_{ji}
    \end{bmatrix}}_{n \cross m} $$

\begin{itemize}
    \item $A^T=A'=A^t=$ Transpose of Matrix $A$
\end{itemize}
\subsection{Properties of Transpose Matrices}

\begin{enumerate}[i.]
    \item $(A^T)^T=A $
    \item $(A\pm B)^T=A^T \pm B^T $
    \item $(kA)^T=k \cdot A^T $
    \item $(AB)^T=B^TA^T $, In General, $\left(\displaystyle\prod_{r=1}^{n} A_{r}\right)^T=\displaystyle\prod_{r=1}^{n} A^T_{n+1-r} $
    \item $I^T=I $
\end{enumerate}
\section{Complex Conjugate of a Matrix}
The Matrix obtained by replacing each element by its complex conjugate.

$$A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{m \cross n}, \overline{A}={\begin{bmatrix}
                \overline{a_{ij}}
            \end{bmatrix}}_{m \cross n} $$

\begin{itemize}
    \item If all elements are $\mathbb{R} $, then $A=\overline{A}$
    \item $\overline{\overline{A}}=A$
    \item $\overline{A+B}=\overline{A}+\overline{B}$
    \item $\overline{kA}=\overline{k} \cdot \overline{A} \hfill k \in \mathbb{C} $
    \item $\overline{AB}=\overline{A} \, \overline{B} $
\end{itemize}

\section{Conjugate Transpose of a Matrix}
The complex conjugate of the transpose of a matrix.

$$A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{m \cross n}, A^{\theta} =\overline{A^T}={\begin{bmatrix}
        \overline{a_{ji}}
    \end{bmatrix}}_{n \cross m} $$

If $A$ and $B$ are two matrices of same order, then
\begin{enumerate}[i.]
    \item $\overline{A^T}=\left(\overline{A}\right)^T$
    \item $\left(A^\theta\right)^\theta=A$
    \item $\left(A+B\right)^\theta=A^\theta+B^\theta $
    \item $\left(kA\right)^\theta=\overline{k} A^\theta \hfill k \in \mathbb{C} $
    \item $\left(AB\right)^\theta=B^\theta A^\thetitle $
\end{enumerate}

\section{Adjoint of A Matrix}
The Transpose of Square Matrix formed by its co-factors is its Adjoint.
\newline \newline
Let $A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{n \cross n} $
\newline \newline
then $$adj(A)={\begin{bmatrix}
        C_{ji}
    \end{bmatrix}}_{n \cross n} $$


\subsection{Properties of Adjoint Matrix}
\begin{enumerate}[i.]
    \item $A \cdot adj(A)= adj(A) \cdot A= det(A) I $
    \item $det(adj(A))=\left(det(A)\right)^{n-1}$
    \item $det(\underbrace{adj(adj(adj( \ldots \hspace{1cm})))}_{m \text{ times } adj})=det(A)^{\left(n-1\right)^{m}}$
    \item $\left(adj(A)\right)^T=adj(A^T)$
    \item $adj(adj(A))=(det(A))^{n-2}A$
    \item $adj(kA)=k^{n-1}adj(A) \hfill k \in \mathbb{C}$
    \item $adj(A^m)=(adj(A))^m \hfill m \in \mathbb{N}$
    \item $det(A \cdot adj(A)+kI_{n})=\left(det(A)+k\right)^n$
    \item Adjoint of a Diagonal Matrix is a Diagonal Matrix.

          If $A= {\begin{bmatrix}
                  a & 0 & 0 \\
                  0 & b & 0 \\
                  0 & 0 & c
              \end{bmatrix}} $, then $adj(A)=\begin{bmatrix}
                  bc & 0  & 0  \\
                  0  & ca & 0  \\
                  0  & 0  & ab
              \end{bmatrix}$
    \item $adj(I_{n})=I_{n}$
    \item $adj(adj(A))=\left(det(A)\right)^{n-2}A$
\end{enumerate}

\section{Inverse of a Matrix}
A square matrix of is said to be Invertible, iff its determinant is non zero.
$$det(A)\not=0$$
$$A^{-1}=\dfrac{adj(A)}{det(A)}$$

\subsection{Properties of Inverse Matrix}
\begin{enumerate}[i.]
    \item Every invertible matrix possesses a unique inverse.
    \item $(AB)^{-1}=B^{-1}A^{-1}$
    \item $(A^T)^{-1}=(A^{-1})^T$
    \item $(A^k)^{-1}=(A^{-1})^k=A^{-k} \hfill k \in \mathbb{N}$
    \item $(A^{-1})^{-1}=A$
    \item $I^{-1}=I$
    \item $det(A^{-1})=\dfrac{1}{det(A)}$
    \item Inverse of a digonal matrix is a digonal matrix.

          If $A=\begin{bmatrix}
                  a & 0 & 0 \\
                  0 & b & 0 \\
                  0 & 0 & c
              \end{bmatrix}$ then $A^{-1}=\begin{bmatrix}
                  \dfrac{1}{a} & 0            & 0            \\
                  0            & \dfrac{1}{b} & 0            \\
                  0            & 0            & \dfrac{1}{c}
              \end{bmatrix}$
\end{enumerate}
\section{Elementary Row Operations}
\begin{enumerate}[i.]
    \item The interchange of $i$th row and $j$th row.
          $$R_{i} \leftrightarrow R_{j}$$
    \item Multiplication of $i$th row by a non zero constant $k$.
          $$R_{i} \rightarrow k\cdot R_{i}$$
    \item Addition of $i$th row to the elements of $j$th row multiplied by a non zero constant $k$.
          $$R_{i} \rightarrow R_{i}+ k \cdot R_{j}$$
\end{enumerate}
\section{Gauss Jordan Method for Computing Inverses}
Let $A$ be an invertible matrix. To find inverse of a using Gauss Jordan Method,

\begin{enumerate}[1.]
    \item Create a matrix $G=\begin{bmatrix}
                  A & | & I
              \end{bmatrix}$
    \item Now using elementary operations on $G$ transform $A$ to $I$.
    \item In the process of trasforming $A$ to $I$, $I$ transforms to $A^{-1}$
\end{enumerate}

\section{Matrix Polynomial}
$$f(A)=a_{n}A^n+a_{n-1}A^{n-1}+a_{n-2}A^{n-2}+ \ldots +a_{1}A+a_{0}I$$

For $n \ge 2, A$ should be a square matrix.

\begin{itemize}
    \item If $det(A) \not= 0, a_{0}\not=0, a_{0}I+a_{1}A+a_{2}A^2+ \ldots + a_{n}A^n=O$,

          then $$A^{-1}=\dfrac{1}{a_{0}}\left(
              a_{1}I+a_{2}A+a_{3}A^2+ \ldots + a_{n}A^{n-1}
              \right)$$
\end{itemize}

\section{Linear System of Equations}
Consider,
$$a_{11}x_{1}+a_{12}x_{2}+a_{13}x_{3}+ \ldots + a_{1n}x_{n}=b_{1}$$
$$a_{21}x_{1}+a_{22}x_{2}+a_{23}x_{3}+ \ldots + a_{2n}x_{n}=b_{2}$$
$$a_{31}x_{1}+a_{32}x_{2}+a_{33}x_{3}+ \ldots + a_{3n}x_{n}=b_{3}$$
$\hphantom{1cm} \hspace{2.6cm} \vdots \hspace{1.1cm} \vdots \hspace{1.1cm} \vdots \hspace{1.0cm} \ddots \hspace{0.6cm} \vdots \hspace{1.1cm} \vdots$
$$a_{n1}x_{1}+a_{n2}x_{2}+a_{n3}x_{3}+ \ldots + a_{nn}x_{n}=b_{n}$$

If $b_{i}=0,i\in \left\{x:x\le n, x \in \mathbb{N}\right\}$, then the above system is called Homogenous. \\If not then it is Non-Homogenous.

In Matrix form,

$$\begin{bmatrix}
        a_{11} & a_{12} & a_{13} & \ldots & a_{1n} \\
        a_{21} & a_{22} & a_{23} & \ldots & a_{2n} \\
        a_{31} & a_{32} & a_{33} & \ldots & a_{3n} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & a_{n3} & \ldots & a_{nn} \\
    \end{bmatrix} \begin{bmatrix}
        x_{1}  \\
        x_{2}  \\
        x_{3}  \\
        \vdots \\
        x_{n}  \\
    \end{bmatrix}=\begin{bmatrix}
        b_{1}  \\
        b_{2}  \\
        b_{3}  \\
        \vdots \\
        b_{n}  \\
    \end{bmatrix}$$
$$AX=B$$
$$X=A^{-1}B=\dfrac{adj(A)B}{det(A)}$$
\subsection{Types of Equations}
\begin{enumerate}[I.]
    \item If system is Non-Homogenous
          \begin{enumerate}[i.]
              \item If $det(A)\not=0$, then system possesses a unique solution.
              \item If $det(A)=0, adj(A)B \not= O$, then system possesses no solution.
              \item If $det(A)=0, adj(A)B=O$, then system possesses $\infty$ solutions.
          \end{enumerate}
    \item If system is Homogenous
          \begin{enumerate}[i.]
              \item If $det(A)\not=0$, then system possesses only one trivial solution.
              \item If $det(A)=0$, then system possesses non-trivial solutions and $\infty$ solutions.
              \item If no. of equations $<$ no. of unknowns, then it has non-trivial solution.
          \end{enumerate}
\end{enumerate}
\section{Echelon Form of a Matrix}
A matrix $A$ is said to be in echelon form, if
\begin{enumerate}[i.]
    \item The first non-zero element in each row is $1$.
    \item Every non-zero row is $A$ preceds eveny zero row (If any).
    \item The number of zeroes before the first non-zero element in 1st, 2nd, ... rows should be in increasing order.
\end{enumerate}
\section{Rank of a Matrix}
Rank of a Matrix $A$, $\rho(A)=\textit{Number of non-zero rows in Echelon form of A}$
\subsection{Properties of Rank of Matrices}
Let $A={\begin{bmatrix}
        a_{ij}
    \end{bmatrix}}_{m \cross n}, B={\begin{bmatrix}
                b_{ij}
            \end{bmatrix}}_{m \cross n}$
\begin{enumerate}[i.]
    \item $\rho(A+B)\le \rho(A)+\rho(B)$
    \item $\rho(AB)\le \rho(A) \wedge \rho(AB)\le\rho(B)$
    \item $\rho(A)=\rho(A^T)$
\end{enumerate}
\section{Solutions to Linear System of Equations Using Rank Method}
Consider,
$$a_{11}x_{1}+a_{12}x_{2}+a_{13}x_{3}+ \ldots + a_{1n}x_{n}=b_{1}$$
$$a_{21}x_{1}+a_{22}x_{2}+a_{23}x_{3}+ \ldots + a_{2n}x_{n}=b_{2}$$
$$a_{31}x_{1}+a_{32}x_{2}+a_{33}x_{3}+ \ldots + a_{3n}x_{n}=b_{3}$$
$\hphantom{1cm} \hspace{2.6cm} \vdots \hspace{1.1cm} \vdots \hspace{1.1cm} \vdots \hspace{1.0cm} \ddots \hspace{0.6cm} \vdots \hspace{1.1cm} \vdots$
$$a_{m1}x_{1}+a_{m2}x_{2}+a_{m3}x_{3}+ \ldots + a_{mn}x_{n}=b_{m}$$

In Matrix form,

$$\begin{bmatrix}
        a_{11} & a_{12} & a_{13} & \ldots & a_{1n} \\
        a_{21} & a_{22} & a_{23} & \ldots & a_{2n} \\
        a_{31} & a_{32} & a_{33} & \ldots & a_{3n} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & a_{m3} & \ldots & a_{mn} \\
    \end{bmatrix} \begin{bmatrix}
        x_{1}  \\
        x_{2}  \\
        x_{3}  \\
        \vdots \\
        x_{n}  \\
    \end{bmatrix}=\begin{bmatrix}
        b_{1}  \\
        b_{2}  \\
        b_{3}  \\
        \vdots \\
        b_{m}  \\
    \end{bmatrix}$$
$$AX=B$$

Now, $$C=\begin{bmatrix}
        A \hspace{2mm} B
    \end{bmatrix}=\begin{bmatrix}
        a_{11} & a_{12} & a_{13} & \ldots & a_{1n} & b_{1}  \\
        a_{21} & a_{22} & a_{23} & \ldots & a_{2n} & b_{2}  \\
        a_{31} & a_{32} & a_{33} & \ldots & a_{3n} & b_{3}  \\
        \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
        a_{m1} & a_{m2} & a_{m3} & \ldots & a_{mn} & bm     \\
    \end{bmatrix}$$
$C$ is called augmented matrix.

\subsection{Types of Equations}
\begin{enumerate}[I.]
    \item Consistent System, If $\rho(A)=\rho(C)$
          \begin{enumerate}[i.]
              \item Unique Solution, If $\rho(A)=\rho(C)=n=\textit{no. of unknowns}$
              \item $\infty$ Solutions, If $\rho(A)=\rho(C)=r<n$
          \end{enumerate}
    \item Inconsistent System, If $\rho(A)\not=\rho(C) \implies \textit{no solution}$
\end{enumerate}

\section{Linear Transformations}
\subsection{Reflection Matrices}
\subsubsection{Reflection through axis of $x$}
The Matrix $\begin{bmatrix}
        1 & 0  \\
        0 & -1
    \end{bmatrix}$ describes the reflection of any point $P(x,y)$ through the\\ axis of $x$.

\subsubsection{Reflection through the axis of $y$}
The Matrix $\begin{bmatrix}
        -1 & 0 \\
        0  & 1
    \end{bmatrix}$ describes the reflection of any point $P(x,y)$ through the\\ axis of $y$.

\subsubsection{Reflection through the origin}
The Matrix $\begin{bmatrix}
        -1 & 0  \\
        0  & -1
    \end{bmatrix}$ describes the reflection of any point $P(x,y)$ through the \\ origin.

\subsubsection{Reflection through line $y=x$}
The matrix $\begin{bmatrix}
        0 & 1 \\
        1 & 0
    \end{bmatrix}$ describes the reflection of any point $P(x,y)$ through the line $y=x$

\subsubsection{Reflection through the line $y=x\tan \theta$}
The matrix $\begin{bmatrix}
        \cos 2 \theta & \sin 2 \theta  \\
        \sin 2 \theta & -\cos 2 \theta
    \end{bmatrix}$ describes the reflection of any point $P(x,y)$ through the line $y=x\tan \theta $

\subsubsection{Rotation Through Angle $\theta$}
The Matrix $\begin{bmatrix}
        \cos \theta & -\sin \theta \\
        \sin \theta & \cos \theta
    \end{bmatrix}$ describes a rotation of a vector through an angle $\theta$.

\section{Eigen Vectors and Eigen Values}
Let $\nu$ be any non-zero vector satisfying,

$$A\nu=\lambda \nu$$

This scalar $\lambda$ is called the Eigen Value (Characteristic Root) and the vector $\nu$ is the Eigen vector (Characteristic Vector).
\subsection{Important Properties of Eigen Values}
\begin{enumerate}[i.]
    \item Any square matrix $A$ and its transpose $A^T$ have the same Eigen Values.
    \item The sum of Eigen Values of a matrix is equal to its trace.
    \item The product of Eigen Values is of a matrix $A$ is equal to its determinant.
    \item If $\lambda_{i}$ are the Eigen Value of $A$, then the Eigen Values of
          \begin{enumerate}[a.]
             \item $kA$ are $k\lambda_{i}$
             \item $A^m$ are ${\lambda_{i}}^m$
             \item $A^{-1}$ are $\lambda_{i}^{-1}$
          \end{enumerate}
    \item All Eigen Values of a real symmetric matrix are $\mathbb{R}$ and the eigen vectors corresponding to two distinct eigen values are orthogonal.
    \item All Eigen Values of a real skew-symmetric matrix are purely imaginary or zero. 
    \item An odd order skew-symmetric matrix is singular and hence has zero as an Eigen value.
    \item If sum of entries in every row or column is a constant, then that constant value is an eigen value of that matrix.
\end{enumerate}
\section{Characteristic Equation}
$$det(A-\lambda I)=0$$
is called characteristic equation of matrix $A_{n \cross n}$.

The adove equation reduces to a $n$th degree polynomial equation in $\lambda$.

\section{Cayley Hamilton Theorem}
Every square matrix $A$ satisfies its characteristic equation $det(A-\lambda I)=0$

$$a_{0}+a_{1}\lambda+\ldots a_{n}\lambda^n=0$$
By Cayley Hamilton Theorem, $\lambda \to A$
$$a_{0}I+a_{1}A+a_{2}A^2+\ldots+a_{n}A^n=O$$
\end{document}